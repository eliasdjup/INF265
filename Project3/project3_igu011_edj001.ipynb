{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Sequence models\n",
    "\n",
    "igu011 and edj001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import torchtext\n",
    "from os import listdir\n",
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the dataset:    1368807\n",
      "Number of distinct words in the dataset: 30374\n",
      "Size the defined vocabular:              1050\n",
      "occurences:\n",
      " [(251055, '<unk>'), (89904, ','), (71106, 'the'), (63121, '.'), (43426, 'and'), (33952, 'to'), (30061, 'of'), (23575, 'a'), (18657, 'in'), (20755, 'he'), (16814, 'that'), (15056, 'was'), (14400, 'his'), (13815, 'it'), (10997, 'with'), (10735, 'had'), (9430, 'her'), (9334, 'not'), (10562, 'you'), (9198, 'as'), (9152, 'at'), (8447, 'him'), (8457, 'is'), (8269, 'for'), (7824, 'on'), (7122, '!'), (6510, '?'), (8087, 'she'), (6477, 's'), (6223, 'be'), (5871, 'said'), (8103, 'but'), (6221, 'all'), (5689, 'have'), (5137, 'from'), (4752, 'which'), (4648, 'me'), (5138, 'so'), (4722, 'by'), (4476, 'were'), (4807, 'my'), (4989, 'this'), (4952, 'they'), (4399, 'one'), (4023, 'who'), (4656, 'what'), (3253, 'up'), (4196, 'there'), (3211, 'them'), (4125, 'we'), (3206, 'would'), (3258, 'an'), (3196, 'are'), (3091, 'been'), (3009, 'or'), (2941, 'out'), (2985, 'will'), (3912, 'when'), (3587, 'no'), (2656, 'could'), (2631, 'man'), (3321, 'if'), (2606, 'did'), (2580, 'their'), (2504, 'into'), (2723, 'do'), (2239, 'more'), (2385, 'your'), (2603, 'now'), (2286, 'only'), (2180, 'very'), (2032, 'time'), (2013, 'about'), (3231, 'then'), (2031, 'some'), (1879, 'went'), (1830, 'came'), (1894, 'see'), (1824, 'down'), (1841, 'has'), (1779, 'know'), (1751, 'himself'), (1738, 't'), (1757, 'little'), (1743, 'before'), (1713, 'can'), (1668, 'again'), (1598, 'like'), (1566, 'must'), (1546, 'over'), (1511, 'eyes'), (1493, 'us'), (1641, 'go'), (1458, 'other'), (1460, 'am'), (1542, 'our'), (1470, 'old'), (1434, 'away'), (1413, 'face'), (1938, 'how'), (1416, 'should'), (1610, 'come'), (1457, 'two'), (1344, 'room'), (1337, 'thought'), (1305, 'than'), (1298, 'back'), (1342, 'without'), (1276, 'any'), (1316, 'such'), (1448, 'where'), (1589, 'after'), (1234, 'made'), (1217, 'way'), (1230, 'shall'), (1235, 'men'), (1304, 'good'), (1177, 'say'), (1180, 'its'), (1154, 'looked'), (1169, 'day'), (1214, 'still'), (1145, 'hand'), (1193, 'first'), (1130, 'saw'), (1129, 'same'), (1114, 'asked'), (1112, 'seemed'), (1117, 'took'), (1110, 'own'), (1195, 'may'), (1089, 'head'), (1092, 'left'), (1075, 'began'), (1080, 'long'), (1076, 'upon'), (1166, 'nothing'), (1143, 'even'), (1257, 'just'), (1091, 'life'), (1056, 'off'), (1127, 'another'), (1030, 'felt'), (1031, 'much'), (1021, 'door'), (1219, 'here'), (1042, 'those'), (1066, 'never'), (1003, 'something'), (1073, 'these'), (985, 'think'), (963, 'too'), (949, 'heard'), (1608, 'well'), (1003, 'once'), (952, 'through'), (945, 'last'), (921, 'being'), (946, 'great'), (912, 'house'), (973, 'take'), (953, 'look'), (846, 'right'), (840, 'might'), (841, 'young'), (863, 'get'), (834, 'night'), (827, 'whole'), (833, 'round'), (821, 'found'), (837, 'quite'), (816, 'make'), (814, 'things'), (790, 'told'), (857, 'tell'), (769, 'knew'), (782, 'always'), (764, 'moment'), (771, 'people'), (841, 'though'), (804, 'dear'), (816, 'yet'), (988, 'let'), (740, 'put'), (710, '('), (710, ')'), (773, 'father'), (799, 'while'), (772, 'every'), (697, 'turned'), (695, 'done'), (686, 'gave'), (693, 'wife'), (816, 'having'), (695, 'love'), (671, 'place'), (701, 'give'), (706, 'because'), (662, 'going'), (655, 'got'), (663, 'whom'), (663, 'woman'), (659, 'army'), (652, 'morning'), (663, 'under'), (639, 'sat'), (663, 'most'), (636, 'seen'), (681, 'three'), (619, 'herself'), (619, 'also'), (630, 'friend'), (607, 'voice'), (608, 'anything'), (623, 'looking'), (603, 'side'), (638, 'soon'), (760, 'don'), (621, 'already'), (595, 'find'), (580, 'stood'), (599, 'mind'), (578, 'words'), (609, 'behind'), (649, 'everything'), (573, 'thing'), (558, 'hands'), (550, 'll'), (541, 'cried'), (543, 'understand'), (543, 'against'), (538, 'answered'), (558, 'many'), (552, 'others'), (536, 'home'), (530, 'alone'), (528, 'part'), (526, 'few'), (525, 'matter'), (523, 'ran'), (537, 'end'), (512, 'want'), (514, 'set'), (605, 'suddenly'), (562, 'mother'), (502, 'brought'), (503, 'heart'), (551, 'each'), (941, 'why'), (539, 'both'), (500, 'ever'), (487, 'myself'), (490, 'along'), (485, 'smile'), (562, 'general'), (499, 'between'), (480, 'taken'), (474, 'work'), (463, 'replied'), (959, 'count'), (463, 'passed'), (462, 'cannot'), (466, 'son'), (507, 'new'), (458, 'white'), (464, 'husband'), (459, 'far'), (453, 'small'), (458, 'together'), (452, 'feeling'), (452, 'officer'), (452, 'became'), (450, 'called'), (465, 'death'), (449, 'word'), (456, 'open'), (454, 'till'), (441, 'light'), (441, 'lay'), (459, 'leave'), (1025, 'princess'), (438, 'better'), (434, 'gone'), (426, 'table'), (425, 'window'), (446, 'does'), (425, 'evening'), (417, 'bed'), (2003, 'prince'), (415, 'world'), (414, 'continued'), (412, 'days'), (413, 'full'), (448, 'poor'), (434, 'order'), (404, 'front'), (402, 'remained'), (403, 'case'), (421, 'horse'), (408, 'ask'), (407, 'coming'), (396, 'fell'), (425, 'evidently'), (401, 'fire'), (390, 'sent'), (432, 'since'), (392, 'near'), (399, 'nor'), (377, 'themselves'), (378, 'saying'), (387, 'speak'), (495, 'king'), (402, 'whether'), (461, 'however'), (370, 'longer'), (377, 'help'), (375, 'power'), (368, 'battle'), (493, 'countess'), (367, 'given'), (377, 'soldiers'), (367, 'train'), (369, 'fear'), (364, 'position'), (361, 'spoke'), (361, 'sleep'), (360, 'wish'), (356, 'ready'), (353, 'fellow'), (353, 'wanted'), (352, 'entered'), (350, 'horses'), (351, 'yourself'), (366, 'letter'), (346, 'possible'), (417, 'next'), (343, 'opened'), (346, 'sitting'), (341, 'expression'), (340, 'soul'), (340, 'hair'), (339, 'years'), (336, 'question'), (367, 'really'), (349, 'strange'), (336, 'air'), (367, 'among'), (336, 'hear'), (333, 'held'), (333, 'lost'), (336, 'money'), (337, 'talk'), (337, 'dead'), (332, 'met'), (335, 'happened'), (335, 'almost'), (325, 'course'), (326, 'rest'), (324, 'able'), (330, 'happy'), (322, 'returned'), (340, 'road'), (346, 'black'), (320, 'child'), (318, 'girl'), (324, 'rose'), (323, 'taking'), (315, 'wished'), (317, 'large'), (330, 'half'), (359, 'lady'), (310, 'best'), (309, 'stopped'), (326, 'keep'), (321, 'true'), (308, 'present'), (316, 'terrible'), (306, 'itself'), (308, 'past'), (339, 'war'), (325, 'believe'), (324, 'beside'), (305, 'sound'), (303, 'received'), (307, 'blood'), (301, 'body'), (302, 'enemy'), (301, 'tried'), (308, 'enough'), (310, 'red'), (300, 'kind'), (299, 'moved'), (303, 'forward'), (298, 'daughter'), (300, 'feel'), (304, 'standing'), (299, 'beautiful'), (299, 'reason'), (298, 'times'), (297, 'arms'), (294, 'clock'), (293, 'business'), (300, 'commander'), (295, 'free'), (295, 'dark'), (292, 'followed'), (290, 'feet'), (290, 'action'), (289, 'else'), (292, 'country'), (286, 'added'), (286, 'become'), (293, 'chief'), (286, 'less'), (286, 'line'), (293, 'read'), (302, 'water'), (285, 'cause'), (284, 'hour'), (284, 'close'), (289, 'rather'), (290, 'within'), (278, 'kept'), (275, 'need'), (298, 'toward'), (299, 'seeing'), (274, 'shouted'), (288, 'until'), (273, 'cold'), (272, 'doing'), (276, 'name'), (270, 'fact'), (275, 'run'), (271, 'troops'), (269, 'crowd'), (282, 'either'), (280, 'turning'), (269, 'ground'), (273, 'afraid'), (270, 'answer'), (272, 'certain'), (267, 'lips'), (265, 'arm'), (265, 'anyone'), (263, 'doubt'), (341, 'during'), (273, 'whose'), (270, 'cut'), (274, 'second'), (264, 'live'), (261, 'silent'), (262, 'soldier'), (260, 'drew'), (263, 'sure'), (268, 'turn'), (354, 'o'), (257, 'rode'), (257, 'understood'), (256, 'idea'), (254, 'appeared'), (256, 'children'), (295, 'doctor'), (254, 'noticed'), (253, 'across'), (263, 'around'), (298, 'sir'), (254, 'talking'), (250, 'thousand'), (249, 'waiting'), (251, 'dinner'), (249, 'clear'), (249, 'return'), (252, 'silence'), (256, 'above'), (254, 'impossible'), (245, 'minutes'), (263, 'bring'), (257, 'women'), (244, 'officers'), (244, 'making'), (352, 'perhaps'), (246, 'late'), (246, 'short'), (243, 'drawing'), (243, 'news'), (248, 'tears'), (239, 'used'), (241, 'boy'), (242, 'pleasure'), (238, 'reached'), (242, 'bad'), (237, 'grew'), (237, 'movement'), (327, 'nature'), (236, 'carried'), (258, 'four'), (236, 'quickly'), (965, 'yes'), (235, 'care'), (277, 'later'), (237, 'says'), (234, 'sight'), (234, 'wounded'), (237, 'high'), (272, 'everyone'), (232, 'struck'), (231, 'led'), (271, 'master'), (231, 'necessary'), (233, 'hope'), (234, 'use'), (275, 'remember'), (243, 'brother'), (276, 'several'), (228, 've'), (226, 'manner'), (228, 'seems'), (227, 'thinking'), (227, 'carriage'), (224, 'sort'), (230, 'killed'), (224, 'strength'), (222, 'hundred'), (230, 'orders'), (225, 'regiment'), (235, 'state'), (222, 'known'), (224, 'point'), (229, 'hard'), (220, 'laid'), (227, 'often'), (225, 'speaking'), (227, 'third'), (7, '-'), (219, 'least'), (219, 'usual'), (221, 'pale'), (217, 'different'), (238, 'fine'), (217, 'smiled'), (228, 'call'), (215, 'closed'), (217, 'town'), (215, 'corner'), (215, 'steps'), (226, 'won'), (214, 'asleep'), (213, 'seem'), (212, 'family'), (212, 'meet'), (213, 'conversation'), (257, 'indeed'), (212, 'arrived'), (227, 'immediately'), (211, 'raised'), (210, 'loved'), (211, 'truth'), (231, 'five'), (213, 'glad'), (210, 'heavy'), (209, 'attention'), (236, 'neither'), (206, 'repeated'), (204, 'sun'), (204, 'threw'), (204, 'change'), (203, 'listened'), (206, 'trying'), (202, 'remarked'), (202, 'snow'), (388, 'm'), (203, 'show'), (197, 'duty'), (196, 'showed'), (195, 'tone'), (194, 'force'), (195, 'smiling'), (197, 'straight'), (208, 'ten'), (214, 'pass'), (197, 'service'), (216, 'company'), (194, 'strong'), (200, 'try'), (192, 'village'), (191, 'merely'), (192, 'presence'), (199, 'towards'), (190, 'caught'), (213, 'd'), (190, 'mouth'), (193, 'study'), (188, 'deep'), (188, 'die'), (191, 'eat'), (192, 'gold'), (202, 'hardly'), (188, 'low'), (192, 'story'), (186, 'placed'), (187, 'reply'), (186, 'walked'), (186, 'desire'), (194, 'kill'), (233, 'wait'), (188, 'wood'), (190, 'de'), (183, 'latter'), (183, 'mean'), (182, 'exclaimed'), (182, 'frightened'), (182, 'glanced'), (182, 'ordered'), (189, 'sister'), (182, 'thin'), (184, 'fall'), (181, 'former'), (180, 'instant'), (183, 'remain'), (180, 'watch'), (180, 'angry'), (179, 'cry'), (180, 'faces'), (182, 'running'), (181, 'smoke'), (179, 'trouble'), (192, 'someone'), (180, 'big'), (177, 'happiness'), (178, 'human'), (177, 'lived'), (178, 'lying'), (177, 'neck'), (177, 'shoulders'), (218, 'certainly'), (177, 'earth'), (178, 'holding'), (175, 'comes'), (175, 're'), (175, 'fresh'), (184, 'hold'), (175, 'surprise'), (177, 'thoughts'), (176, 'events'), (174, 'leaving'), (172, 'coat'), (173, 'knows'), (173, 'pretty'), (173, 'voices'), (171, 'chair'), (171, 'glass'), (172, 'important'), (170, 'fixed'), (171, 'simply'), (194, 'therefore'), (182, 'history'), (169, 'interest'), (206, 'sometimes'), (170, 'married'), (174, 'station'), (166, 'flew'), (168, 'getting'), (166, 'opinion'), (206, 'please'), (166, 'act'), (167, 'dress'), (165, 'drove'), (164, 'account'), (164, 'ago'), (180, 'beyond'), (164, 'hours'), (166, 'living'), (165, 'move'), (169, 'bear'), (167, 'contrary'), (166, 'moving'), (164, 'questions'), (164, 'year'), (162, 'broken'), (162, 'floor'), (162, 'kissed'), (165, 'ought'), (166, 'peace'), (161, 'effort'), (161, 'forest'), (168, 'passing'), (162, 'pleased'), (162, 'quiet'), (162, 'unable'), (162, 'affairs'), (172, 'field'), (161, 'giving'), (160, 'subject'), (164, 'adjutant'), (159, 'laughed'), (160, 'legs'), (158, 'sides'), (160, 'wolf'), (160, 'foot'), (158, 'friends'), (157, 'papers'), (157, 'started'), (160, 'blue'), (158, 'danger'), (159, 'joy'), (157, 'seized'), (156, 'beginning'), (155, 'box'), (156, 'command'), (158, 'especially'), (169, 'outside'), (154, 'handsome'), (177, 'none'), (154, 'carry'), (157, 'bird'), (157, 'following'), (154, 'further'), (157, 'honor'), (154, 'step'), (152, 'view'), (166, 'castle'), (150, 'common'), (151, 'letters'), (150, 'shook'), (151, 'wind'), (150, 'expected'), (160, 'six'), (165, 'society'), (158, 'bridge'), (150, 'evil'), (148, 'remembered'), (148, 'sad'), (154, 'staff'), (147, 'means'), (151, 'police'), (148, 'rushed'), (147, 'asking'), (146, 'ball'), (151, 'calm'), (146, 'changed'), (149, 'pain'), (146, 'wrong'), (145, 'eye'), (145, 'sake'), (152, 'stay'), (144, 'surprised'), (151, 'chance'), (143, 'distance'), (147, 'freedom'), (150, 'stone'), (143, 'matters'), (142, 'person'), (142, 'purpose'), (159, 'sit'), (155, 'write'), (142, 'bright'), (143, 'gentleman'), (142, 'died'), (140, 'driver'), (143, 'figure'), (140, 'glance'), (150, 'quick'), (143, 'real'), (140, 'single'), (140, 'spot'), (142, 'follow'), (139, 'occurred'), (145, 'save'), (140, 'fallen'), (138, 'ladies'), (151, 'send'), (153, 'seven'), (143, 'suppose'), (139, 'drawn'), (138, 'finished'), (147, 'law'), (139, 'particularly'), (141, 'walk'), (137, 'wrote'), (136, 'happen'), (142, 'knowing'), (137, 'paper'), (137, 'pity'), (140, 'tree'), (135, 'affair'), (135, 'covered'), (135, 'direction'), (136, 'express'), (136, 'occupied'), (192, 'thus'), (136, 'begin'), (136, 'journey'), (134, 'simple'), (136, 'written'), (133, 'broke'), (133, 'decided'), (138, 'garden'), (135, 'touched'), (132, 'considered'), (158, 'court'), (134, 'dog'), (132, 'experienced'), (132, 'formed'), (133, 'plan'), (132, 'spite'), (140, 'afterwards'), (132, 'laughing'), (134, 'marry'), (138, 'number'), (132, 'pressed'), (196, 'captain'), (132, 'dressed'), (130, 'expressed'), (130, 'meant'), (132, 'peasant'), (167, 'whatever'), (130, 'whispered'), (129, 'bent'), (131, 'forget'), (129, 'hat'), (129, 'interrupted'), (130, 'peasants'), (129, 'fast'), (131, 'looks'), (131, 'middle'), (128, 'minute'), (128, 'spent'), (128, 'wall'), (128, 'wild'), (133, 'clearly'), (128, 'easy'), (128, 'ill'), (127, 'knife'), (131, 'laugh'), (127, 'piece'), (127, 'sky'), (127, 'attack'), (130, 'excellency'), (126, 'meaning'), (127, 'pleasant'), (126, 'sense'), (128, 'drink'), (128, 'except'), (130, 'imagine'), (132, 'according'), (124, 'caused'), (124, 'ceased'), (124, 'effect'), (128, 'post'), (124, 'prepared'), (126, 'seat'), (127, 'tall'), (142, 'early'), (123, 'engine'), (124, 'knowledge'), (128, 'meeting'), (125, 'rapidly'), (123, 'rooms'), (132, 'search'), (125, 'visit'), (123, 'waited'), (123, 'clothes'), (123, 'difficult'), (123, 'notice'), (123, 'prisoners'), (122, 'quietly'), (122, 'result'), (145, 'stop'), (123, 'telling'), (133, 'to-night'), (122, 'darkness'), (141, 'everybody'), (132, 'golden'), (122, 'maid'), (121, 'serious'), (121, 'burning'), (122, 'farther'), (120, 'finger'), (121, 'form'), (125, 'hall'), (120, 'inquired'), (120, 'sprang'), (120, 'crime'), (119, 'shoulder'), (119, 'throat'), (119, 'enter'), (123, 'gate'), (118, 'teeth'), (119, 'unknown'), (118, 'week'), (117, 'activity'), (124, 'blow'), (118, 'deal'), (118, 'dying'), (117, 'empty'), (118, 'historians'), (117, 'pointed'), (117, 'talked'), (117, 'anxious'), (116, 'ears'), (116, 'gazed'), (124, 'land'), (122, 'lie'), (116, 'play'), (121, 'twenty'), (117, 'wine'), (115, 'ourselves'), (115, 'path'), (130, 'sea'), (116, 'settled'), (116, 'spoken'), (114, 'aim'), (114, 'future'), (114, 'greater'), (140, 'listen'), (114, 'marriage'), (114, 'paused'), (114, 'shouting'), (116, 'stand'), (113, 'aside'), (113, 'beauty'), (113, 'fingers'), (114, 'learned'), (125, 'military'), (116, 'mine'), (121, 'note'), (113, 'particular'), (115, 'ship'), (206, 'street'), (142, 'whilst'), (116, 'youth'), (112, 'alive'), (112, 'approached'), (113, 'campaign'), (112, 'cap'), (112, 'disappeared'), (114, 'sign'), (114, 'sounds'), (112, 'lit'), (119, 'river'), (116, 'shut'), (112, 'sweet'), (111, 'thrown'), (111, 'watched'), (131, 'city'), (110, 'grown'), (112, 'regard'), (111, 'understanding'), (112, 'various'), (109, 'condition'), (145, 'consider'), (109, 'makes'), (112, 'satisfied'), (111, 'slowly'), (110, 'start'), (126, 'tomorrow'), (109, 'windows'), (118, 'below'), (108, 'evident'), (108, 'fight'), (110, 'horror'), (121, 'reading'), (113, 'ring'), (109, 'servants'), (108, 'wide'), (107, 'bit'), (140, 'colonel'), (107, 'event'), (115, 'fate'), (109, 'filled'), (107, 'habit'), (112, 'hearing'), (126, 'instead'), (107, 'lifted'), (110, 'murder'), (109, 'sorry'), (108, 'special'), (109, 'warm'), (106, 'fit'), (106, 'handed'), (109, 'health'), (106, 'passage'), (106, 'pocket'), (108, 'shot'), (106, 'shown'), (107, 'suffering'), (107, 'terror'), (105, 'begun'), (109, 'drive'), (107, 'experience'), (107, 'key'), (106, 'lies'), (105, 'party'), (108, 'secret'), (104, 'advanced'), (104, 'burst'), (104, 'facts'), (105, 'girls'), (107, 'learn'), (108, 'nearer'), (105, 'report'), (104, 'sofa'), (103, 'appear'), (103, 'appearance'), (104, 'breath'), (105, 'continually'), (103, 'difficulty'), (103, 'dream'), (103, 'explain'), (104, 'food'), (103, 'galloped'), (103, 'gesture'), (104, 'growing'), (103, 'length'), (103, 'miles'), (103, 'passion'), (103, 'places'), (103, 'slept'), (106, 'spirit'), (121, 'twice'), (103, 'carefully'), (116, 'clever'), (104, 'escape'), (102, 'forehead'), (102, 'guns'), (105, 'hussars'), (107, 'nearly'), (102, 'presented'), (103, 'putting'), (102, 'stepped'), (102, 'uniform'), (104, 'worse'), (103, 'anger'), (117, 'hill'), (101, 'liked'), (105, 'listening'), (103, 'tea'), (100, 'age'), (100, 'bound'), (100, 'character'), (100, 'couple'), (100, 'crossed'), (102, 'generals'), (112, 'kindly'), (100, 'natural'), (111, 'probably'), (100, 'pushed'), (100, 'safe'), (103, 'supper'), (16404, 'i')]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer will split a long text into a list of english words\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def read_files(datapath='./'):\n",
    "    \"\"\"\n",
    "    Return a list of strings, one for each line in each .txt files in 'datapath'\n",
    "    \"\"\"\n",
    "    # Find all txt files in directory \n",
    "    files = listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith(\".txt\")]\n",
    "\n",
    "    \n",
    "    # Stores each line of each book in a list\n",
    "    lines = []\n",
    "    for f_name in files:\n",
    "        with open(f_name) as f:\n",
    "            lines += f.readlines()\n",
    "    return lines\n",
    "\n",
    "books = read_files(datapath='./data_train/')\n",
    "\n",
    "# Match any word containing digit\n",
    "no_digits = '\\w*[0-9]+\\w*'\n",
    "# Match word containing a uppercase \n",
    "no_names = '\\w*[A-Z]+\\w*'\n",
    "# Match any sequence containing more than one space\n",
    "no_spaces = '\\s+'\n",
    "\n",
    "\n",
    "def tokenize(lines):\n",
    "    \"\"\"\n",
    "    Tokenize the list of lines\n",
    "    \"\"\"\n",
    "    list_text = []\n",
    "    for line in lines:\n",
    "        list_text += tokenizer(line)\n",
    "    return list_text\n",
    "\n",
    "def yield_tokens(lines):\n",
    "    \"\"\"\n",
    "    Yield tokens, ignoring names and digits to build vocabulary\n",
    "    \"\"\"\n",
    "    for line in lines:\n",
    "        line = re.sub(no_digits + \"|\" + no_names, ' ', line)\n",
    "        line = re.sub(no_spaces, ' ', line)\n",
    "        yield tokenizer(line)\n",
    "\n",
    "def count_freqs(data, vocab):\n",
    "    \"\"\"\n",
    "    Count occurrences of each word in vocabulary in the data\n",
    "    \"\"\"\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in data:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs\n",
    "\n",
    "# List of words contained in the dataset\n",
    "list_words = tokenize(books)\n",
    "\n",
    "# vocab contains the vocabulary found in the data, associating an index to each word\n",
    "vocab = build_vocab_from_iterator(yield_tokens(books), min_freq=100, specials=[\"<unk>\"])\n",
    "# Since we removed all words with an uppercase when building the vocabulary, we skipped the word \"I\"\n",
    "vocab.append_token(\"i\")\n",
    "\n",
    "# Value of default index. This index will be returned when OOV (Out Of Vocabulary) token is queried.\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Total number of words in the dataset:   \", len(list_words))\n",
    "print(\"Number of distinct words in the dataset:\", len(set(list_words)))\n",
    "print(\"Size the defined vocabular:             \", vocab_size)\n",
    "\n",
    "\n",
    "freqs = count_freqs(list_words, vocab)\n",
    "print(\"occurences:\\n\", [(f.item(), w) for (f, w)  in zip(freqs, vocab.lookup_tokens(range(vocab_size)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (/home/elias/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1204175)",
      "at /home/elias/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (/home/elias/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1223212)",
      "at v.dispose (/home/elias/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:1216694)",
      "at /home/elias/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533674",
      "at t.swallowExceptions (/home/elias/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:913059)",
      "at dispose (/home/elias/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:533652)",
      "at t.RawSession.dispose (/home/elias/.vscode-server/extensions/ms-toolsai.jupyter-2022.3.1000901801/out/extension.js:2:537330)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "def create_dataset(\n",
    "    text, vocab, context_size=3, \n",
    "):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    n_text = len(text)\n",
    "    n_vocab = len(vocab)\n",
    "    \n",
    "    # Transform the text as a list of integers.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    for i in range(n_text - context_size):\n",
    "        \n",
    "        # true label = 'is the next word a known word (i.e. not '<unk>' token)?'\n",
    "        t = int(txt[i + context_size] != 0)\n",
    "            \n",
    "        # Context before\n",
    "        c = txt[i:i + context_size]\n",
    "            \n",
    "        targets.append(t)\n",
    "        # Normally we should use word embedding, and not hot encoding, but we \n",
    "        # skip that part for this exercise\n",
    "        contexts.append(F.one_hot(torch.tensor(c), num_classes=n_vocab))\n",
    "            \n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets)\n",
    "    return TensorDataset(contexts, targets)\n",
    "\n",
    "data = create_dataset(list_words, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Conjugating *be* and *have*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Text generation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2237f2d220ad310471ebc7a3ba2d52d32f1f45a06155c1490d637beab9bd187"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('INF265_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

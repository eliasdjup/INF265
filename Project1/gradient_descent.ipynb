{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from datetime import datetime\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "else torch.device('cpu'))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1\n",
    "Load and preprocess the CIFAR-10 dataset. Split it into 3 datasets: training, validation and\n",
    "test. Take a subset of these datasets by keeping only 2 labels: bird and plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of the train dataset:         45000\n",
      "Size of the validation dataset:    5000\n",
      "Size of the test dataset:          10000\n",
      "Size of the training dataset:  9017\n",
      "Size of the validation dataset:  983\n",
      "Size of the test dataset:  2000\n"
     ]
    }
   ],
   "source": [
    "def load_cifar(train_val_split=0.9, data_path='../data/', preprocessor=None):\n",
    "    \n",
    "    # Define preprocessor if not already given\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))\n",
    "        ])\n",
    "    \n",
    "    # load datasets\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,      \n",
    "        train=True,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path, \n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # train/validation split\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val =  len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, \n",
    "        [n_train, n_val],\n",
    "        generator=torch.Generator().manual_seed(123)\n",
    "    )\n",
    "\n",
    "    print(\"Size of the train dataset:        \", len(data_train))\n",
    "    print(\"Size of the validation dataset:   \", len(data_val))\n",
    "    print(\"Size of the test dataset:         \", len(data_test))\n",
    "    \n",
    "    return (data_train, data_val, data_test)\n",
    "\n",
    "cifar10_train, cifar10_val, cifar10_test = load_cifar()\n",
    "\n",
    "# Now define a lighter version of CIFAR10: cifar\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "# For each dataset, keep only airplanes and birds\n",
    "cifar2_train = [(img, label_map[label]) for img, label in cifar10_train if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]\n",
    "cifar2_test = [(img, label_map[label]) for img, label in cifar10_test if label in [0, 2]]\n",
    "\n",
    "print('Size of the training dataset: ', len(cifar2_train))\n",
    "print('Size of the validation dataset: ', len(cifar2_val))\n",
    "print('Size of the test dataset: ', len(cifar2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2\n",
    "Write a MyMLP class that implements a MLP in PyTorch (so only fully connected layers) such\n",
    "that:\n",
    "(a) The input dimension is 3072 (= 32*32*3) and the output dimension is 2 (for the 2\n",
    "classes).\n",
    "(b) The hidden layers have respectively 512, 128 and 32 hidden units.\n",
    "(c) All activation functions are ReLU. The last layer has no activation function since the\n",
    "cross-entropy loss already includes a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  \n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        # 32*32*3: determined by our dataset: 32x32 RGB images\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.act3 = nn.ReLU()\n",
    "        # 2: determined by our number of classes (birds and planes)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.flat(x)\n",
    "        out = self.act1(self.fc1(out))\n",
    "        out = self.act2(self.fc2(out))\n",
    "        out = self.act3(self.fc3(out))\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3\n",
    "Write a train(n epochs, optimizer, model, loss fn, train loader) function that trains\n",
    "model for n epochs epochs given an optimizer optimizer, a loss function loss fn and a dat-\n",
    "aloader train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device=device, dtype=torch.double)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4\n",
    "Write a similar function train manual update that has no optimizer parameter, but a learn-\n",
    "ing rate lr parameter instead and that manually updates each trainable parameter of model\n",
    "using equation (3). Do not forget to zero out all gradients after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, lr, model, loss_fn, train_loader, verbose = False):\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device=device, dtype=torch.double)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # optimizer step\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= lr * p.grad\n",
    "            \n",
    "            # zero out all gradients\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p.grad.zero_()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or verbose:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5\n",
    "Train 2 instances of MyMLP, one using train and the other using train manual update (use\n",
    "the same parameter values for both models). Compare their respective training losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:02:36.585104  |  Epoch 1  |  Training loss 0.645\n",
      "11:02:46.574501  |  Epoch 10  |  Training loss 0.326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6451217197440591,\n",
       " 0.5329839468263902,\n",
       " 0.4778253794609127,\n",
       " 0.4468125583284399,\n",
       " 0.4197904609343939,\n",
       " 0.3968746333461311,\n",
       " 0.3773816702722962,\n",
       " 0.3595321877492997,\n",
       " 0.3426136703937787,\n",
       " 0.32593414376821933]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = MyMLP().to(device=device) \n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False)\n",
    "n_epochs = 10\n",
    "train(n_epochs, optimizer, model, nn.CrossEntropyLoss(), train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:02:47.830246  |  Epoch 1  |  Training loss 0.645\n",
      "11:02:58.042466  |  Epoch 10  |  Training loss 0.326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.645121719744059,\n",
       " 0.5329839468263902,\n",
       " 0.4778253794609127,\n",
       " 0.44681255832843997,\n",
       " 0.4197904609343939,\n",
       " 0.3968746333461311,\n",
       " 0.3773816702722962,\n",
       " 0.3595321877492997,\n",
       " 0.34261367039377877,\n",
       " 0.3259341437682193]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = MyMLP().to(device=device) \n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False)\n",
    "n_epochs = 10\n",
    "train_manual_update(n_epochs, 1e-2, model, nn.CrossEntropyLoss(), train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the train and train_manual_update function produces the same training losses when given the same data and learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6\n",
    "\n",
    "Modify train manual update by adding a L2 regularization term in your manual parameter\n",
    "update. Add an additional weight decay parameter to train manual update. Compare\n",
    "again train and train manual update results with 0 < weight decay < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7\n",
    "\n",
    "Modify train manual update by adding a momentum term in your parameter update. Add\n",
    "an additional momentum parameter to train manual update. Check again the correctness of\n",
    "the new update rule by comparing it to train function (with 0 < momentum < 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.8\n",
    "\n",
    "Train different instances (at least 4) of the MyMLP model with different learning rate, momentum\n",
    "and weight decay values . You can choose the same values as in the\n",
    "gradient descent output.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.9\n",
    "Select the best model among those trained in the previous question based on their accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.10\n",
    "Evaluate the best model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2237f2d220ad310471ebc7a3ba2d52d32f1f45a06155c1490d637beab9bd187"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('INF265_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
